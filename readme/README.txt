# cuda_ailocal

#### About this addon

It is a simple plugin for `Cudatext` that can be connected to Ollama to use an LLM locally.

#### Installation

Download the repository content and unzip it in the `cudatext\py` folder.
It must be inside a folder: `cuda_AILocal`

#### Configuration

- Ctrl+Shift+P
- AI LocalConfig

**Note:** for the moment only tested with Ollama

![videoDemo.gif](videoDemo.gif)

---

# cuda_ailocal

#### Sobre este addon

Es un plugin sencillo para `Cudatext` que se puede conectar a Ollama para utilizar un LLM de forma local.

#### Instalación

Bajar el contenido del repositorio y descomprimir en la carpeta `cudatext\py`
Debe estar dentro de una carpeta: `cuda_AILocal`

#### Configuración

- Ctrl+Shift+P
- AI Local\Config

> **Nota:** por el momento solo probado con Ollama

Author: Walter Wagner

License: MIT