# cuda_AILocal

#### Sobre este addon

Es un plugin sencillo para `Cudatext` que se puede conectar a Ollama para utilizar un LLM de forma local.

#### Instalación

Bajar el contenido del repositorio y descomprimir en la carpeta `cudatext\py`
Debe estar dentro de una carpeta: `cuda_AILocal`

#### Configuración

- Ctrl+Shift+P
- AI Local\Config

**Nota:** por el momento solo probado con Ollama

---

# cuda_AILocal

#### About this addon

It is a simple plugin that can be connected to Ollama to use a LLM locally.

#### Installation

Download the repository content and unzip it in the `cudatext\py` folder.
It must be inside a folder: `cuda_AILocal`

#### Configuration

- Ctrl+Shift+P
- AI LocalConfig

**Note:** for the moment only tested with Ollama

![videoDemo.gif](videoDemo.gif)

Author: Walter Wagner
License: MIT